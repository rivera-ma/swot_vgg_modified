{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: swath_unit_vectors\n",
    "* input: lon, lat grids in SWOT LR ocean height files\n",
    "* output: $\\frac{\\partial e}{\\partial a}$, $\\frac{\\partial n}{\\partial a}$, $\\frac{\\partial e}{\\partial c}$, $\\frac{\\partial n}{\\partial c}$\n",
    "* east slope: $s_e = \\frac{\\partial h}{\\partial e}$, <br>\n",
    "north slope: $s_n = \\frac{\\partial h}{\\partial n}$, <br>\n",
    "cross-track slope: $s_c = \\frac{\\partial h}{\\partial c}$, <br>\n",
    "along-track slope: $s_a = \\frac{\\partial h}{\\partial a}$. <br>\n",
    "* $\\begin{bmatrix} s_c \\\\ s_a \\end{bmatrix} = \n",
    "\\begin{bmatrix} \\frac{\\partial e}{\\partial c} & \\frac{\\partial n}{\\partial c} \\\\ \\frac{\\partial e}{\\partial a} & \\frac{\\partial n}{\\partial a} \\end{bmatrix} \n",
    "\\begin{bmatrix} s_e \\\\ s_n \\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función, ``swath_unit_vectors``, se utiliza para calcular vectores unitarios en las direcciones a lo largo de la trayectoria (along-track) y transversales a la trayectoria (cross-track) en un conjunto de datos que representan un área geográfica, como los datos satelitales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swath_unit_vectors(lon, lat):\n",
    "    \"\"\"\n",
    "    compute the unit direction vectors for each cell in a swath\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat : array_like\n",
    "        2D array of latitude values with shape (na, nc).\n",
    "    lon : array_like\n",
    "        2D array of longitude values with shape (na, nc).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    uea : array_like\n",
    "        2D array of the east component of the init vector in the along-track direction\n",
    "    una : array_like\n",
    "        2D array of the north component of the init vector in the along-track direction\n",
    "    uec : array_like\n",
    "        2D array of the east component of the init vector in the cross-track\n",
    "    unc : array_like\n",
    "        2D array of the north component of the init vector in the cross-track\n",
    "    \"\"\"\n",
    "    d2r = np.pi/180.0\n",
    "    cost = np.cos(lat*d2r)\n",
    "    lon = np.unwrap(lon, period = 360)\n",
    "\n",
    "    dna, dnc = np.gradient(lat,1,1)\n",
    "    dea, dec = np.gradient(lon,1,1)*cost\n",
    "    dsa = (dea*dea + dna*dna)**0.5\n",
    "    uea = dea/dsa\n",
    "    una = dna/dsa\n",
    "    dsc = (dec*dec + dnc*dnc)**0.5\n",
    "    uec = dec/dsc\n",
    "    unc = dnc/dsc\n",
    "\n",
    "    return uea, una, uec, unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: model_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código implementa un modelo que calcula la pendiente (slope) en distintas direcciones (a lo largo de la trayectoria y transversal, así como en las direcciones norte y este) a partir de datos de altimetría del satélite SWOT en resolución baja (L2 LR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    input: SWOT L2 LR file name of one file; id of pass (odd-ascending, even-descending)\n",
    "#    model slope in the along-track, cross-track, north, east directions\n",
    "def model_slope(multibeam_expert, pass_id):\n",
    "\n",
    "    # read one file to get the lon/lat \n",
    "    ds_multibeam = xr.open_dataset(multibeam_expert)\n",
    "    num_lines = ds_multibeam.sizes['num_lines']\n",
    "    num_pixels = ds_multibeam.sizes['num_pixels']\n",
    "    # get unit vectors in the along-track an cross-track direction\n",
    "    [uea, una, uer, unr] = swath_unit_vectors(ds_multibeam.longitude.values, ds_multibeam.latitude.values)\n",
    "    lon = ds_multibeam[\"longitude\"].values\n",
    "    lon = np.unwrap(lon, period = 360)\n",
    "    lat = ds_multibeam[\"latitude\"].values\n",
    "    # create model cross-track slope\n",
    "    data = {'longitude': lon.flatten(),\n",
    "            'latitude': lat.flatten()}\n",
    "    da = 2000\n",
    "    dr = 2000\n",
    "    mdt_grid = \"datos/dot.cls21.grd\"\n",
    "    east_slope_grid = \"datos/east_32.1.nc\"\n",
    "    north_slope_grid = \"datos/north_32.1.nc\"\n",
    "    track_points = pd.DataFrame(data)\n",
    "    # Use grdtrack to sample the grid along the track\n",
    "    track_data = pygmt.grdtrack(points=track_points, grid=east_slope_grid,newcolname=\"east_slope\")\n",
    "    north_data = pygmt.grdtrack(points=track_points, grid=north_slope_grid,newcolname=\"north_slope\")\n",
    "    mdt_data = pygmt.grdtrack(points=track_points, grid=mdt_grid,newcolname=\"mdt_model\")\n",
    "    track_data['north_slope'] = north_data['north_slope']\n",
    "    track_data['mdt_model'] = mdt_data['mdt_model']\n",
    "    # convert to 2d numpy\n",
    "    east_model = track_data.east_slope.to_numpy().reshape(num_lines, num_pixels)\n",
    "    north_model = track_data.north_slope.to_numpy().reshape(num_lines, num_pixels)\n",
    "\n",
    "    sa_mdt, sr_mdt = np.gradient(track_data.mdt_model.to_numpy().reshape(num_lines, num_pixels), da, dr)\n",
    "    if pass_id%2==1:\n",
    "        sa_mdt = -sa_mdt\n",
    "        sr_mdt = -sr_mdt\n",
    "    determinant = (uer*una-uea*unr) # all ones\n",
    "    sn_mdt = -uea*sr_mdt+uer*sa_mdt\n",
    "    se_mdt = una*sr_mdt-unr*sa_mdt\n",
    "    sa_mdt *= 1000000\n",
    "    sr_mdt *= 1000000\n",
    "    sn_mdt *= 1000000\n",
    "    se_mdt *= 1000000\n",
    "\n",
    "    sa_model = uea*east_model+una*north_model + sa_mdt\n",
    "    sr_model = uer*east_model+unr*north_model + sr_mdt\n",
    "    \n",
    "    if pass_id%2==1: # ascending\n",
    "        sa_model = uea*east_model+una*north_model+sa_mdt\n",
    "        sr_model = uer*east_model+unr*north_model+sr_mdt\n",
    "        return sa_model, sr_model, north_model+sn_mdt, east_model+se_mdt\n",
    "\n",
    "    else: # descending\n",
    "        sa_model = -1*(uea*east_model+una*north_model+sa_mdt)\n",
    "        sr_model = -1*(uer*east_model+unr*north_model+sr_mdt)\n",
    "        return sa_model, sr_model, north_model-sn_mdt, east_model-se_mdt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: Process one pass (old version)\n",
    "output to lon, lat, north slope, east slope, VGG, residual north slope, residual east slope, residual VGG, to a pandas dataframe <br>\n",
    "here we use mean to get the slope residuals, can update codes to use median which is resilient to outliers<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    input: SWOT L2 LR file names of the same pass (list); pass_id(integer);\n",
    "#    output: sa, sr, sn, se\n",
    "def process_onepass_old(list_fnames, pass_id):\n",
    "\n",
    "    # read one file to get the lon/lat \n",
    "    num_files = len(list_fnames)\n",
    "    if num_files > 1:\n",
    "        [sa_model, sr_model, sn_model, se_model] = model_slope(list_fnames[0], pass_id)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # read all files\n",
    "    sa_list = []\n",
    "    sr_list = []\n",
    "    ssh_list = []\n",
    "    swh_list = []\n",
    "    for i in range(num_files):\n",
    "        ds_expert = xr.open_dataset(list_fnames[i])\n",
    "        num_lines = ds_expert.sizes['num_lines']\n",
    "        num_pixels = ds_expert.sizes['num_pixels']\n",
    "        distance = ds_expert.cross_track_distance.values\n",
    "        ssha = ds_expert.ssha_karin_2\n",
    "        ssha = np.where(ds_expert.ssha_karin_2_qual == 0, ssha, np.nan)\n",
    "        ssha = np.where(ds_expert.ancillary_surface_classification_flag == 0, ssha, np.nan)\n",
    "        ssh = ssha + ds_expert.mean_sea_surface_cnescls.values+ds_expert.height_cor_xover\n",
    "        swh = ds_expert.swh_karin\n",
    "        swh = np.where(ds_expert.ssha_karin_2_qual == 0, swh, np.nan)\n",
    "        swh = np.where(swh != 0, swh, np.nan)\n",
    "        da = 2000\n",
    "        dr = 2000\n",
    "        sa, sr = np.gradient(ssh, da, dr)\n",
    "        sa = sa * 10**6 # along-track ssh slope in microradian\n",
    "        sr = sr * 10**6 # cross-track ssh slope in microradian\n",
    "        dsr = np.nanmean(sr - sr_model) #remove offset\n",
    "        sr = sr - dsr\n",
    "        sa_list.append(sa)\n",
    "        sr_list.append(sr)\n",
    "        swh_list.append(swh)\n",
    "\n",
    "    # 1/swh as weight\n",
    "    weight = np.nan_to_num(1/np.array(swh_list), nan=0.0)\n",
    "    weight[weight<(1/6.0)] = 0 # if SWH>6.0m, do not use the data\n",
    "    masked_data = np.ma.masked_array(np.array(sa_list), np.isnan(np.array(sa_list)))\n",
    "    sa_stack = np.ma.average(masked_data, axis=0, weights=weight).filled(np.nan)\n",
    "    masked_data = np.ma.masked_array(np.array(sr_list), np.isnan(np.array(sr_list)))\n",
    "    sr_stack = np.ma.average(masked_data, axis=0, weights=weight).filled(np.nan)\n",
    "    if pass_id%2==1: # ascending\n",
    "        sa_stack = -sa_stack\n",
    "        sr_stack = -sr_stack\n",
    "        saa, sar = np.gradient(sa_stack, da, dr)\n",
    "        sra, srr = np.gradient(sr_stack, da, dr)\n",
    "        vgg = -(saa + srr)*9.80665*1000 # Eotvos\n",
    "\n",
    "        saa_m, sar_m = np.gradient(sa_model, da, dr)\n",
    "        sra_m, srr_m = np.gradient(sr_model, da, dr)\n",
    "        vgg_model = -(saa_m + srr_m)*9.80665*1000  # Eotvos\n",
    "        \n",
    "    else: #descending\n",
    "        saa, sar = np.gradient(sa_stack, da, dr)\n",
    "        sra, srr = np.gradient(sr_stack, da, dr)\n",
    "        vgg = (saa + srr)*9.80665*1000 # Eotvos\n",
    "\n",
    "        saa_m, sar_m = np.gradient(sa_model, da, dr)\n",
    "        sra_m, srr_m = np.gradient(sr_model, da, dr)\n",
    "        vgg_model = (saa_m + srr_m)*9.80665*1000  # Eotvos\n",
    "        \n",
    "    vgg_res = vgg - vgg_model\n",
    "    # remove model slope first\n",
    "    masked_data = np.ma.masked_array(sr_stack - sr_model, np.isnan(sr_stack - sr_model))\n",
    "    sr_diff = np.ma.average(masked_data, axis=0).filled(np.nan)\n",
    "        \n",
    "    # apply phasescreen correction\n",
    "    phasescreen = np.array([sr_diff] * num_lines)\n",
    "    sr_stack_corrected = sr_stack - phasescreen\n",
    "\n",
    "    # get unit vectors in the along-track an cross-track direction\n",
    "    [uea, una, uer, unr] = swath_unit_vectors(ds_expert.longitude.values, ds_expert.latitude.values)\n",
    "    sn_stack = (-uea*sr_stack_corrected+uer*sa_stack)\n",
    "    se_stack = (una*sr_stack_corrected-unr*sa_stack)\n",
    "    if pass_id%2==0: # descending\n",
    "        sn_stack = -sn_stack\n",
    "        se_stack = -se_stack\n",
    "    sn_res = sn_stack - sn_model\n",
    "    se_res = se_stack - se_model\n",
    "\n",
    "    # create output pandas dataframe\n",
    "    out = {'longitude': ds_expert[\"longitude\"].values.flatten(),\n",
    "            'latitude': ds_expert[\"latitude\"].values.flatten(),\n",
    "          'sn_stack':sn_stack.flatten(),\n",
    "          'se_stack':se_stack.flatten(),\n",
    "          'vgg': vgg.flatten(),\n",
    "          'sn_res':sn_res.flatten(),\n",
    "          'se_res':se_res.flatten(),\n",
    "          'vgg_res': vgg_res.flatten()}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: Process one pass \n",
    "output to lon, lat, residual north slope, east slope, VGG to a pandas dataframe <br>\n",
    "here we use mean to get the slope residuals, can update codes to use median which is resilient to outliers<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    input: SWOT L2 LR file names of the same pass (list); pass_id(integer);\n",
    "#    output: sa, sr, sn, se\n",
    "def process_onepass(list_fnames, pass_id):\n",
    "\n",
    "    # read one file to get the lon/lat \n",
    "    num_files = len(list_fnames)\n",
    "    if num_files > 1:\n",
    "        [sa_model, sr_model, sn_model, se_model] = model_slope(list_fnames[0], pass_id)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # read all files\n",
    "    sa_list = []\n",
    "    sr_list = []\n",
    "    ssh_list = []\n",
    "    swh_list = []\n",
    "    for i in range(num_files):\n",
    "        ds_expert = xr.open_dataset(list_fnames[i])\n",
    "        num_lines = ds_expert.sizes['num_lines']\n",
    "        num_pixels = ds_expert.sizes['num_pixels']\n",
    "        distance = ds_expert.cross_track_distance.values\n",
    "        ssha = ds_expert.ssha_karin_2\n",
    "        ssha = np.where(ds_expert.ssha_karin_2_qual == 0, ssha, np.nan)\n",
    "        ssha = np.where(ds_expert.ancillary_surface_classification_flag == 0, ssha, np.nan)\n",
    "        ssha = np.where(ds_expert.distance_to_coast.values > 2000, ssha, np.nan)\n",
    "        ssh = ssha + ds_expert.mean_sea_surface_cnescls.values+ds_expert.height_cor_xover\n",
    "        swh = ds_expert.swh_karin\n",
    "        swh = np.where(ds_expert.ssha_karin_2_qual == 0, swh, np.nan)\n",
    "        swh = np.where(swh != 0, swh, np.nan)\n",
    "\n",
    "        da = 2000\n",
    "        dr = 2000\n",
    "        sa, sr = np.gradient(ssh, da, dr)\n",
    "        sa = sa * 10**6 # along-track ssh slope in microradian\n",
    "        sr = sr * 10**6 # cross-track ssh slope in microradian\n",
    "        dsr = np.nanmean(sr - sr_model)\n",
    "        sr = sr - dsr\n",
    "        sa_list.append(sa)\n",
    "        sr_list.append(sr)\n",
    "        swh_list.append(swh)\n",
    "\n",
    "    # uniform weight\n",
    "    weight = np.nan_to_num(0*np.array(swh_list)+1, nan=1)\n",
    "    weight[np.array(swh_list)>6] = 0 # if SWH>6.0m, do not use the data\n",
    "    masked_data = np.ma.masked_array(np.array(sa_list), np.isnan(np.array(sa_list)))\n",
    "    sa_stack = np.ma.average(masked_data, axis=0, weights=weight).filled(np.nan)\n",
    "    masked_data = np.ma.masked_array(np.array(sr_list), np.isnan(np.array(sr_list)))\n",
    "    sr_stack = np.ma.average(masked_data, axis=0, weights=weight).filled(np.nan)\n",
    "\n",
    "    if pass_id%2==1: # ascending\n",
    "        sa_stack = -sa_stack\n",
    "        sr_stack = -sr_stack\n",
    "        \n",
    "    # apply phasescreen correction\n",
    "    masked_data = np.ma.masked_array(sr_stack - sr_model, np.isnan(sr_stack - sr_model))\n",
    "    sr_diff = np.ma.average(masked_data, axis=0).filled(np.nan) \n",
    "    phasescreen = np.array([sr_diff] * num_lines)\n",
    "    sr_stack_corrected = sr_stack - phasescreen\n",
    "\n",
    "    sa_stack_res = sa_stack - sa_model\n",
    "    sr_stack_corrected_res = sr_stack_corrected - sr_model\n",
    "    if pass_id%2==1: # ascending\n",
    "        saa_res, sar_res = np.gradient(sa_stack_res, da, dr)\n",
    "        sra_res, srr_res = np.gradient(sr_stack_corrected_res, da, dr)\n",
    "        vgg_res = -(saa_res + srr_res)*9.80665*1000 # Eotvos\n",
    "    if pass_id%2==0: # descending\n",
    "        saa_res, sar_res = np.gradient(sa_stack_res, da, dr)\n",
    "        sra_res, srr_res = np.gradient(sr_stack_corrected_res, da, dr)\n",
    "        vgg_res = (saa_res + srr_res)*9.80665*1000 # Eotvos\n",
    "\n",
    "    # get unit vectors in the along-track an cross-track direction\n",
    "    [uea, una, uer, unr] = swath_unit_vectors(ds_expert.longitude.values, ds_expert.latitude.values)\n",
    "    sn_stack = (-uea*sr_stack_corrected+uer*sa_stack)\n",
    "    se_stack = (una*sr_stack_corrected-unr*sa_stack)\n",
    "    if pass_id%2==0: # descending\n",
    "        sn_stack = -sn_stack\n",
    "        se_stack = -se_stack\n",
    "    sn_res = sn_stack - sn_model\n",
    "    se_res = se_stack - se_model\n",
    "\n",
    "    # create output pandas dataframe\n",
    "    out = {'longitude': ds_expert[\"longitude\"].values.flatten(),\n",
    "            'latitude': ds_expert[\"latitude\"].values.flatten(),\n",
    "           'sn_res':sn_res.flatten(),\n",
    "           'se_res':se_res.flatten(),\n",
    "           'vgg_res': vgg_res.flatten()}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function: Process one pass and also output the rms\n",
    "output to lon, lat, residual north slope, east slope, VGG, and the L2, L1 norm of VGG, to a pandas dataframe <br>\n",
    "here we use median to get the slope residuals, can update codes to use mean<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_onepass_rms(list_fnames, pass_id):\n",
    "\n",
    "    # read one file to get the lon/lat \n",
    "    num_files = len(list_fnames)\n",
    "    if num_files > 1:\n",
    "        [sa_model, sr_model, sn_model, se_model] = model_slope(list_fnames[0], pass_id)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # read all files\n",
    "    sa_list = []\n",
    "    sr_list = []\n",
    "    swh_list = []\n",
    "    for i in range(num_files):\n",
    "        ds_expert = xr.open_dataset(list_fnames[i])\n",
    "        num_lines = ds_expert.sizes['num_lines']\n",
    "        num_pixels = ds_expert.sizes['num_pixels']\n",
    "        distance = ds_expert.cross_track_distance.values\n",
    "        ssha = ds_expert.ssha_karin_2\n",
    "        ssha = np.where(ds_expert.ssha_karin_2_qual == 0, ssha, np.nan)\n",
    "        ssha = np.where(ds_expert.swh_karin.values < 6, ssha, np.nan)\n",
    "        ssha = np.where(ds_expert.ancillary_surface_classification_flag == 0, ssha, np.nan)\n",
    "        ssha = np.where(ds_expert.distance_to_coast.values > 2000, ssha, np.nan)\n",
    "        ssh = ssha + ds_expert.mean_sea_surface_cnescls.values+ds_expert.height_cor_xover\n",
    "        swh = ds_expert.swh_karin\n",
    "        swh = np.where(ds_expert.ssha_karin_2_qual == 0, swh, np.nan)\n",
    "        swh = np.where(swh != 0, swh, np.nan)\n",
    "\n",
    "        da = 2000\n",
    "        dr = 2000\n",
    "        sa, sr = np.gradient(ssh, da, dr)\n",
    "        sa = sa * 10**6 # along-track ssh slope in microradian\n",
    "        sr = sr * 10**6 # cross-track ssh slope in microradian\n",
    "        dsr = np.nanmean(sr - sr_model)\n",
    "        sr = sr - dsr\n",
    "        sa_list.append(sa)\n",
    "        sr_list.append(sr)\n",
    "        swh_list.append(swh)\n",
    "\n",
    "    # uniform weight\n",
    "    weight = np.nan_to_num(0*np.array(swh_list)+1, nan=1)\n",
    "    weight[np.array(swh_list)>6] = 0 # if SWH>6.0m, do not use the data\n",
    "    weight[np.isnan(np.array(sa_list))] = 0 # if sa = nan, do not use the data\n",
    "    weight[np.isnan(np.array(sr_list))] = 0 # if sr = nan, do not use the data\n",
    "    masked_data = np.ma.masked_array(np.array(sa_list), np.isnan(np.array(sa_list)))\n",
    "    # sa_stack = np.ma.average(masked_data, axis=0, weights=weight).filled(np.nan)\n",
    "    sa_stack = np.ma.median(masked_data, axis=0).filled(np.nan)\n",
    "    masked_data = np.ma.masked_array(np.array(sr_list), np.isnan(np.array(sr_list)))\n",
    "    # sr_stack = np.ma.average(masked_data, axis=0, weights=weight).filled(np.nan)\n",
    "    sr_stack = np.ma.median(masked_data, axis=0).filled(np.nan)\n",
    "\n",
    "    point_used = np.sum((weight!=0), axis = 0)\n",
    "\n",
    "    # remove pixels with only a few cycles collected\n",
    "    sa_stack[point_used<8] = np.nan\n",
    "    sr_stack[point_used<8] = np.nan\n",
    "\n",
    "    if pass_id%2==1: # ascending\n",
    "        sa_stack = -sa_stack\n",
    "        sr_stack = -sr_stack\n",
    "        \n",
    "    # apply phasescreen correction\n",
    "    masked_data = np.ma.masked_array(sr_stack - sr_model, np.isnan(sr_stack - sr_model))\n",
    "    sr_diff = np.ma.average(masked_data, axis=0).filled(np.nan) \n",
    "    phasescreen = np.array([sr_diff] * num_lines)\n",
    "    sr_stack_corrected = sr_stack - phasescreen\n",
    "\n",
    "    sa_stack_res = sa_stack - sa_model\n",
    "    sr_stack_corrected_res = sr_stack_corrected - sr_model\n",
    "\n",
    "    # calculate VGG from the stacked slope\n",
    "    if pass_id%2==1: # ascending\n",
    "        saa_res, sar_res = np.gradient(sa_stack_res, da, dr)\n",
    "        sra_res, srr_res = np.gradient(sr_stack_corrected_res, da, dr)\n",
    "        vgg_res = -(saa_res + srr_res)*9.80665*1000 # Eotvos\n",
    "    if pass_id%2==0: # descending\n",
    "        saa_res, sar_res = np.gradient(sa_stack_res, da, dr)\n",
    "        sra_res, srr_res = np.gradient(sr_stack_corrected_res, da, dr)\n",
    "        vgg_res = (saa_res + srr_res)*9.80665*1000 # Eotvos\n",
    "\n",
    "    # get unit vectors in the along-track an cross-track direction\n",
    "    [uea, una, uer, unr] = swath_unit_vectors(ds_expert.longitude.values, ds_expert.latitude.values)\n",
    "    sn_stack = (-uea*sr_stack_corrected+uer*sa_stack)\n",
    "    se_stack = (una*sr_stack_corrected-unr*sa_stack)\n",
    "    if pass_id%2==0: # descending\n",
    "        sn_stack = -sn_stack\n",
    "        se_stack = -se_stack\n",
    "    sn_res = sn_stack - sn_model\n",
    "    se_res = se_stack - se_model\n",
    "\n",
    "    # calculate VGG from each cycle\n",
    "    vgg_list = []\n",
    "    for i in range(num_files):\n",
    "        if pass_id%2==1: # ascending\n",
    "            saa_res, sar_res = np.gradient(-sa_list[i] - sa_model, da, dr)\n",
    "            sra_res, srr_res = np.gradient(-sr_list[i] - phasescreen - sr_model, da, dr)\n",
    "            vgg = -(saa_res + srr_res)*9.80665*1000 # Eotvos\n",
    "            vgg_list.append(vgg)\n",
    "        if pass_id%2==0: # descending\n",
    "            saa_res, sar_res = np.gradient(sa_list[i] - sa_model, da, dr)\n",
    "            sra_res, srr_res = np.gradient(sr_list[i] - phasescreen - sr_model, da, dr)\n",
    "            vgg = (saa_res + srr_res)*9.80665*1000 # Eotvos\n",
    "            vgg_list.append(vgg)\n",
    "\n",
    "\n",
    "    # calculate rms \n",
    "    masked_data = np.ma.masked_array(np.array(vgg_list)-vgg_res, np.isnan(np.array(vgg_list)-vgg_res))\n",
    "    vgg_rms = np.ma.var(masked_data, axis=0).filled(np.nan)\n",
    "    vgg_rms = vgg_rms**0.5\n",
    "    masked_data = np.ma.masked_array(np.abs(np.array(vgg_list)-vgg_res), np.isnan(np.array(vgg_list)-vgg_res))\n",
    "    vgg_L1 = np.ma.median(masked_data, axis=0).filled(np.nan)\n",
    "\n",
    "\n",
    "    # create output pandas dataframe\n",
    "    out = {'longitude': ds_expert[\"longitude\"].values.flatten(),\n",
    "            'latitude': ds_expert[\"latitude\"].values.flatten(),\n",
    "           'sn_res':sn_res.flatten(),\n",
    "           'se_res':se_res.flatten(),\n",
    "           'vgg_res': vgg_res.flatten(),\n",
    "           'vgg_rms': vgg_rms.flatten(),\n",
    "           'vgg_L1': vgg_L1.flatten()}\n",
    "\n",
    "    return out\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
